# LSDobserve
lsdobserve:
  clusterType: "%%CLUSTER_TYPE%%" # openshift | gke | rancher
  # adminUsername is the general username to configure for all tools, so each tool uses the same username
  adminUsername: "lsdobserve"
  # adminPsername is the general password to configure for all tools, so each tool uses the same password
  adminPassword: "password4lsdobserve"
  # the domain to use as a suffix for all ingress traffic. Example grafana-lsd-observe.$domain
  domain: "%%APP_DOMAIN%%"
  smtp:
    host: "%%SMTP%%"
    port: "%%SMTP_PORT%%"
    username: "XXXXX"
    password: "XXXXX"
    fromAddress: "lsdobserve+noreply+%%APP_DOMAIN%%@%%DOMAIN%%"
    fromName: "LSDobserve - %%CLIENT%% - %%APP_DOMAIN%%@%%DOMAIN%%"
  letsencrypt: false
  # letsencrypt-staging for staging
  # letsencrypt for production
  letsencryptIssuer: letsencrypt-staging
  grafana:
    enabled: true
    ingress:
      url: grafana.%%APP_DOMAIN%%
  elastic:
    enabled: true
    count: "3"
    storage: "50Gi"
    storageClassName: "%%STORAGE_CLASS%%"
    filebeat:
      enabled: true
      image: "docker.elastic.co/beats/filebeat:7.10.0"
    apm:
      enabled: true
    kibana:
      enabled: true
      count: "2"
      ingress:
        url: kibana.%%APP_DOMAIN%%

# Elasticsearch Exporter
# To get a latest values you can run:
# helm show values prometheus-community/prometheus-elasticsearch-exporter
prometheus-elasticsearch-exporter:
  replicaCount: 0

# Elasticsearch Exporter
# To get a latest values you can run:
# helm show values elastic/eck-operator
eck-operator:
  image:
    # repository is the container image prefixed by the registry name.
    # skopeo copy --all docker://docker.elastic.co/eck/eck-operator:1.3.0 docker://my.local.registry:5000/eck/eck-operator:1.3.0
    repository: docker.elastic.co/eck/eck-operator
    pullPolicy: IfNotPresent
    tag: none 

  replicaCount: 1
  # installCRDs must be disabled
  installCRDs: false
  resources:
    limits:
      cpu: 200m
      memory: 512Mi
    requests:
      cpu: 20m
      memory: 150Mi

# Grafana
# To get a latest values you can run:
# helm show values grafana/grafana
grafana:
  adminUser: admin
  adminPassword: pho0iar5zo2oob8geevohd0ahr4hieNi
  replicas: 1
  # ingress is disable because it is created via the lsd-observe template
  ingress:
    enabled: false
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 10m
      memory: 128Mi
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: LSDobserve - Prometheus
          type: prometheus
          url: http://lsdobserve-prometheus-server
          isDefault: true
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: "ds-01"
          orgId: 1
          folder: "LSDobserve"
          type: file
          disableDeletion: false
          editable: false
          options:
            path: /var/lib/grafana/dashboards/ds-01
  dashboardsConfigMaps:
    ds-01: "grafana-dashboard-kubernetes-overview"
    ds-02: "grafana-dashboard-namespace-details"
    ds-03: "grafana-dashboard-node-namespace-details"
    ds-04: "grafana-dashboard-apiserver"
    ds-05: "grafana-dashboard-rook-ceph-overview"
    ds-06: "node-exporter-for-prometheus-dashboard"
  imageRenderer:
    enabled: true
    replicas: 1
    image:
      repository: grafana/grafana-image-renderer
      tag: latest
    service:
      portName: "http"
      port: 8081
    podPortName: http
    revisionHistoryLimit: 10
    networkPolicy:
      limitIngress: true
      limitEgress: false
    resources:
      limits:
        cpu: 100m
        memory: 100Mi
      requests:
        cpu: 50m
        memory: 50Mi

# Prometheus
# To get a latest values you can run:
# helm show values prometheus-community/prometheus
prometheus:
  server:
    replicaCount: 2
    retention: 31d
    statefulSet:
      enabled: true
    ingress:
      enabled: true
      hosts:
        - prometheus.%%APP_DOMAIN%%
    persistentVolume:
      enabled: true
      accessModes:
        - ReadWriteOnce
      size: 30Gi
    resources:
      limits:
        cpu: 500m
        memory: 2Gi
      requests:
        cpu: 100m
        memory: 512Mi
  pushgateway:
    enabled: false
  nodeExporter:
    enabled: true
    hostNetwork: true
    hostPID: true
    name: node-exporter
    pod:
      labels:
        org: lsd
        product: lsdobserve
        name: node-exporter
    resources:
      limits:
        cpu: 200m
        memory: 50Mi
      requests:
        cpu: 10m
        memory: 10Mi
    securityContext:
      {}
      # runAsUser: 0
    service:
      annotations:
        prometheus.io/scrape: "true"
      labels: {}
      clusterIP: None
      externalIPs: []
      hostPort: 8100
      loadBalancerIP: ""
      loadBalancerSourceRanges: []
      servicePort: 8100
      type: ClusterIP
    tolerations:
      - key: "node-role.kubernetes.io/controlplane"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/etcd"
        operator: "Exists"
        effect: "NoExecute"
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
  alertmanager:
    enabled: true
    replicaCount: 3
    prefixURL: "https://prometheus.apps.cluster-01.lsdopen.io"
    baseURL: "https://alertmanager.apps.cluster-01.lsdopen.io"
    service:
      enableMeshPeer: true
    statefulSet:
      enabled: true
      annotations: {}
      labels: {}
      podManagementPolicy: OrderedReady
      headless:
        annotations: {}
        labels: {}
        enableMeshPeer: true
    image:
      repository: prom/alertmanager
      pullPolicy: IfNotPresent
      # Overrides the image tag whose default is the chart appVersion.
      tag: ""

    ingress:
      enabled: true
      hosts:
        - alertmanager.%%APP_DOMAIN%%
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 10m
        memory: 32Mi
    persistentVolume:
      enabled: true
      accessModes:
        - ReadWriteOnce
      size: 1Gi
  alertmanagerFiles:
    alertmanager.yml:
      global:
        resolve_timeout: 5m
      receivers:
        - name: LSD Support
          email_configs:
            - to: support+%%CLIENT%%@lsd.co.za
              from: lsdobserve+noreply+%%APP_DOMAIN%%.%%DOMAIN%%
              smarthost: "%%SMTP%%:%%SMTP_PORT%%"
              require_tls: false
      route:
        group_by:
          - job
        group_interval: 5m
        group_wait: 30s
        receiver: "LSD Support"
        repeat_interval: 12h
        routes:
          - receiver: LSD Support
            match:
              severity: warning
          - receiver: LSD Support
            match:
              severity: critical
  serverFiles:
    alerting_rules.yml:
      groups:
        - name: Instances
          rules:
            - alert: InstanceDown
              expr: up == 0
              for: 5m
              labels:
                severity: warning
              annotations:
                description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes."
                message: "When Instance/Endpoints are marked as down it means Prometheus cannot scrape those targets. You can get more details by gong to the Prometheus frontend, going to Status and Targets. You always want all you Instance/Endpoints up else you will not be able to monitor anything"
        - name: NodesMarkedAsUnscheduled
          rules:
            - alert: NodesMarkedAsUnscheduled
              expr: kube_node_spec_unschedulable > 0
              for: 1h
              labels:
                severity: warning
              annotations:
                description: "{{ $labels.kubernetes_node }} is marked as Unscheduled for longer than 1h minutes."
                message: "When Nodes are marked Unscheduled no new pods will be scheduled "
        - name: NodeMemoryUsageAbove85Percent
          rules:
            - alert: NodeMemoryUsageAbove85Percent
              expr: 100 * (1 - ((avg_over_time(node_memory_MemFree_bytes[2m]) + avg_over_time(node_memory_Cached_bytes[2m]) + avg_over_time(node_memory_Buffers_bytes[2m])) / avg_over_time(node_memory_MemTotal_bytes[2m]))) > 80 < 90
              for: 1h
              labels:
                severity: warning
              annotations:
                description: '{{ $labels.kubernetes_node }} is using {{ printf "%.0f" $value }}% of the total memory'
                message: "When the memory of a node is exhausted pods will be evicted and sacrificed to keep the node ready. It is recommended that you cordon node {{ $labels.kubernetes_node }} and delete a couple of pods on the node, forcing them to start up on another node, then uncordone {{ $labels.kubernetes_node }}. You can find more info here: https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/"
        - name: NodeMemoryUsageAbove95Percent
          rules:
            - alert: NodeMemoryUsageAbove95Percent
              expr: 100 * (1 - ((avg_over_time(node_memory_MemFree_bytes[2m]) + avg_over_time(node_memory_Cached_bytes[2m]) + avg_over_time(node_memory_Buffers_bytes[2m])) / avg_over_time(node_memory_MemTotal_bytes[2m]))) > 95
              for: 1h
              labels:
                severity: critical
              annotations:
                description: '{{ $labels.kubernetes_node }} is using {{ printf "%.0f" $value }}% of the total memory'
                message: "When the memory of a node is exhausted pods will be evicted and sacrificed to keep the node ready. It is recommended that you cordon node {{ $labels.kubernetes_node }} and delete a couple of pods on the node, forcing them to start up on another node, then uncordone {{ $labels.kubernetes_node }}. You can find more info here: https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/"
        - name: PVCUsageOver85Percent
          rules:
            - alert: PVCUsageOver85Percent
              expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 85
              for: 1h
              labels:
                severity: warning
              annotations:
                description: 'PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using {{ printf "%.0f" $value }}% of the capacity'
                message: 'You will need to go into the Pod that is using that PVC and clean up some storage. For example "kubectl -n {{ $labels.namespace }} exec -it PODNAME -- sh"'
        - name: PVCUsageOver95Percent
          rules:
            - alert: PVCUsageOver95Percent
              expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 95
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} is using {{ printf "%.0f" $value }}% of the capacity'
                message: 'You will need to go into the Pod that is using that PVC and clean up some storage. For example "kubectl -n {{ $labels.namespace }} exec -it PODNAME -- sh"'
        - name: NodeFileSystemUsageOver85Percent
          rules:
            - alert: NodeFileSystemUsageOver85Percent
              expr: 100 - ((node_filesystem_avail_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"} * 100) / node_filesystem_size_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"}) > 85
              for: 1h
              labels:
                severity: warning
              annotations:
                description: 'Mount point {{ $labels.mountpoint }} on Node {{ $labels.kubernetes_node }} is at {{ printf "%.0f" $value }}% of total capacity'
                message: "You will need to SSH into Node {{ $labels.kubernetes_node }} and clean up storage on {{ $labels.mountpoint }}"
        - name: NodeFileSystemUsageOver95Percent
          rules:
            - alert: NodeFileSystemUsageOver95Percent
              expr: 100 - ((node_filesystem_avail_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"} * 100) / node_filesystem_size_bytes {mountpoint!~".*/host/.*",mountpoint!~".*/etc/.*",mountpoint!~".*/run/secrets.*",mountpoint!~".*/var/run/.*"}) > 95
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'Mount point {{ $labels.mountpoint }} on Node {{ $labels.kubernetes_node }} is at {{ printf "%.0f" $value }}% of total capacity'
                message: "You will need to SSH into Node {{ $labels.kubernetes_node }} and clean up storage on {{ $labels.mountpoint }}"
        - name: TotalAvailableCPURequestsOver90Percent
          rules:
            - alert: TotalAvailableCPURequestsOver90Percent
              expr: (sum ((sum(kube_pod_container_resource_requests_cpu_cores{container!="deployment",container!="docker-build",namespace!="logging",namespace!="default",namespace!~".*openshift-.*",namespace!~".*openmonitoring.*",namespace!~".*kube-.*"} > 0) by (container,pod) / count(kube_pod_container_status_running > 0) by (container,pod))*1000)) / (sum (kube_node_status_allocatable_cpu_cores)*1000) * 100 > 90
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'The total allowed CPU requests are at {{ printf "%.0f" $value }}%'
                message: "When the total allowed CPU requests hits 100% no more pods will be allowed to start up. You either need to lower the CPU requests of pods or add more CPU into the cluster"
        - name: NodeCpuUtilizationOver95Percent
          rules:
            - alert: NodeCpuUtilizationOver95Percent
              expr: 100 - (avg by (kubernetes_node) (rate( node_cpu_seconds_total {mode="idle"}[2m])) * 100) > 95
              for: 1h
              labels:
                severity: warning
              annotations:
                description: 'Node {{ $labels.kubernetes_node }} has a CPU utlization over 95% for over 1 hour. Current CPU utilization of Node {{ $labels.kubernetes_node }} is {{ printf "%.0f" $value }}%'
                message: "When the CPU is maxed out for over an hour it indicates an that a process is killing the node or the node does not have enough CPU assigned to it"
        - name: NodeLoadOver50
          rules:
            - alert: NodeLoadOver50
              expr: node_load15 > 50
              for: 1h
              labels:
                severity: critical
              annotations:
                description: 'Node {{ $labels.kubernetes_node }} has a 15 minute load average of {{ printf "%.0f" $value }}%'
                message: "When the load is over 50 this indicates an issue of high CPU, low memory (going into swap) and high disk utlization"
